{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"model_lstm.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"13eW4xHjHW1IFH3mg4yNgD3TQIJwG5ywy","authorship_tag":"ABX9TyPN+WPUEOj08euKT9KrXMit"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ou7h92BJXrbG","executionInfo":{"status":"ok","timestamp":1607414608961,"user_tz":-540,"elapsed":4243,"user":{"displayName":"히로","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggye-RKGYCwEq4L84yAntlpVGdcpIrR0aiEf7tGjg=s64","userId":"15899289586719876657"}},"outputId":"4d470865-7217-4831-bf23-fc2db23862b4"},"source":["!pip install \"wheel==0.34.2\""],"execution_count":1,"outputs":[{"output_type":"stream","text":["Collecting wheel==0.34.2\n","  Downloading https://files.pythonhosted.org/packages/8c/23/848298cccf8e40f5bbb59009b32848a4c38f4e7f3364297ab3c3e2e2cd14/wheel-0.34.2-py2.py3-none-any.whl\n","Installing collected packages: wheel\n","  Found existing installation: wheel 0.35.1\n","    Uninstalling wheel-0.35.1:\n","      Successfully uninstalled wheel-0.35.1\n","Successfully installed wheel-0.34.2\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":211},"id":"OWB4oFyIWs66","executionInfo":{"status":"ok","timestamp":1607414716137,"user_tz":-540,"elapsed":104122,"user":{"displayName":"히로","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggye-RKGYCwEq4L84yAntlpVGdcpIrR0aiEf7tGjg=s64","userId":"15899289586719876657"}},"outputId":"8e3b9529-422d-4536-bffe-f739af033d22"},"source":["# Default Code\n","!pip3 install torch torchvision\n","\n","# Code from Colab\n","from os import path\n","from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n","platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n","\n","accelerator = 'cu80' if path.exists('/opt/bin/nvidia-smi') else 'cpu'\n","\n","!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.3.0.post4-{platform}-linux_x86_64.whl torchvision\n","import torch\n","torch.__version__"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.7.0+cu101)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.8.1+cu101)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch) (1.18.5)\n","Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch) (0.8)\n","Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch) (0.16.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch) (3.7.4.3)\n","Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (7.0.0)\n","\u001b[K     |████████████████████████████████| 592.3MB 1.2MB/s \n","\u001b[31mERROR: torchvision 0.8.1+cu101 has requirement torch==1.7.0, but you'll have torch 0.3.0.post4 which is incompatible.\u001b[0m\n","\u001b[31mERROR: fastai 1.0.61 has requirement torch>=1.0.0, but you'll have torch 0.3.0.post4 which is incompatible.\u001b[0m\n","\u001b[?25h"],"name":"stdout"},{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'0.3.0.post4'"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"code","metadata":{"id":"aboB17_dWdRj","executionInfo":{"status":"ok","timestamp":1607414826663,"user_tz":-540,"elapsed":1574,"user":{"displayName":"히로","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggye-RKGYCwEq4L84yAntlpVGdcpIrR0aiEf7tGjg=s64","userId":"15899289586719876657"}}},"source":["# -*- coding:utf-8 -*-\n","\"\"\"\n","本实验基于类似class-oriented 所以效果要好很多\n","基于record-oriented未做，改一下数据集划分便可以。\n","\"\"\"\n","import os\n","import torch\n","from torch import nn\n","from torch.utils.data import Dataset, DataLoader\n","import matplotlib.pyplot as plt\n","import pickle\n","import numpy as np\n","from matplotlib.backends.backend_pdf import PdfPages\n","\n","TRAIN = False  # 训练标志\n","CONTINUE_TRAIN = False  # 接着上次某一次训练结果继续训练\n","TEST = False  # 测试标志 设置成True时候，需要指定加载哪个模型\n","PAPER_TEST = True  # 得到写paper使用的测试指标\n","SAVE_TEST_FIG = True\n","EPOCHS = 100\n","BATCH_SIZE = 32\n","Seqlength = 300\n","NUM_SEGS_CLASS = 5\n","\n","qtdb_pkl = '/content/drive/MyDrive/2020-2 Deep Learning/팀프로젝트/[LAST] ECG-Segment-LSTM-master/qtdb_pkl/'  # 数据预处理后的路径，便于调试网络\n","save_path = '/content/drive/MyDrive/2020-2 Deep Learning/팀프로젝트/[LAST] ECG-Segment-LSTM-master/ckpt/'  # 保存模型的路径\n","\n","\n","if not os.path.exists(save_path):\n","    os.mkdir(save_path)\n","\n","\n","class ECGDataset(Dataset):\n","    \"\"\"ecg dataset.\n","       返回字典：{'signal':  ,'label': }\n","    \"\"\"\n","    def __init__(self, qtdb_pkl, data):\n","        \"\"\"\n","        :param qtdb_pkl: 数据库存放路径\n","        :param data: 训练集和验证集数据\n","        \"\"\"\n","        pkl = os.path.join(qtdb_pkl, data)\n","        with open(pkl, 'rb') as f:\n","            x, y = pickle.load(f)\n","        self.x = x\n","        self.y = y\n","\n","    def __len__(self):\n","        return len(self.x)\n","\n","    def __getitem__(self, idx):\n","        signal = torch.from_numpy(self.x[idx]).float()\n","        label = torch.from_numpy(self.y[idx]).float()\n","        sample = {'signal': signal, 'label': label}\n","        return sample\n","\n","\n","class SegModel(torch.nn.Module):\n","    def __init__(self, input_size, hidden_size, num_layers, out_size):\n","        super().__init__()\n","        self.features = torch.nn.Sequential(\n","            # torch.nn.Linear(in_features=input_size, out_features=hidden_size),\n","            torch.nn.LSTM(input_size=input_size,\n","                          hidden_size=hidden_size,\n","                          num_layers=num_layers,\n","                          batch_first=True,\n","                          bidirectional=True),\n","        )\n","        self.classifier = torch.nn.Sequential(\n","            torch.nn.Linear(2*hidden_size, 2*hidden_size),\n","            torch.nn.ReLU(inplace=True),\n","            torch.nn.Dropout(),\n","\n","            torch.nn.Linear(2 * hidden_size, 2 * hidden_size),\n","            torch.nn.ReLU(inplace=True),\n","            torch.nn.Dropout(),\n","        )\n","        self.output = torch.nn.Linear(2*hidden_size, out_size)\n","\n","    def forward(self, x):\n","        \"\"\"\n","        :param x: shape(batch, seq_len, input_size)\n","        :return:\n","        \"\"\"\n","        batch, seq_len, nums_fea = x.size()\n","        features, _ = self.features(x)\n","        output = self.classifier(features)\n","        output = self.output(output.view(batch * seq_len, -1))\n","        return output\n","\n","\n","def train(net, data_loader, epochs):\n","    for step in range(epochs):\n","        net.train()\n","        for i, samples_batch in enumerate(data_loader):\n","            total = 0.0\n","            correct = 0.0\n","\n","            output = net(samples_batch['signal'])\n","            target = samples_batch['label'].contiguous().view(-1).long()\n","            loss = criterion(output, target)\n","\n","            _, predicted = torch.max(output.data, 1)\n","\n","            total += target.size(0)\n","            correct += (predicted == target).sum().item()\n","\n","            if (i+1) % 20 == 0:\n","                print(\"EPOCHS:{},Iter:{},Loss:{:.4f},Acc:{:.4f}\".format(step, i+1, loss.item(), correct/total))\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","        # 每2个epoch,保存一次模型\n","        if (step+1) % 2 == 0:\n","            torch.save(net, save_path+'epoch_{}.ckpt'.format(step))\n","        test(ecg_train_dl, 'train', step)\n","        test(ecg_val_dl, 'val', step)\n","\n","\n","def test(data_loader, str1, step):\n","    with torch.no_grad():\n","        right = 0.0\n","        total = 0.0\n","        net.eval()\n","        for sample in data_loader:\n","            output = net(sample['signal'])\n","            _, predicted = torch.max(output.data, 1)\n","            label = sample['label'].contiguous().view(-1).long()\n","            total += label.size(0)\n","            right += (predicted == label).sum().item()\n","        print(\"epoch:{},{} ACC: {:.4f}\".format(step, str1, right / total))\n","\n","\n","def restore_net(ckpt):\n","    # load models\n","    with open(ckpt, 'rb') as f:\n","        net = torch.load(f)\n","    return net\n","\n","\n","def get_charateristic(y):\n","    Ppos = Qpos = Rpos =Spos = Tpos = 0\n","    for i, val in enumerate(y):\n","        if val == 1 and y[i-1] == 0:\n","            Ppos = i\n","        if val == 2 and y[i-1] == 0:\n","            Qpos = i\n","        if val == 2 and y[i+1] == 3:\n","            Rpos = i\n","        if val == 3 and y[i+1] == 0:\n","            Spos = i\n","        if val == 4 and y[i-1] == 0:\n","            Tpos = i\n","    return Ppos, Qpos, Rpos, Spos, Tpos\n","\n","\n","def point_equal(label, predict, tolerte):\n","    if predict <= label + tolerte * 250 and predict >= label- tolerte * 250:\n","        return True\n","    else:\n","        return False\n","\n","\n","def right_point(label_tuple, predict_tuple, tolerte):\n","    n = np.array([0, 0, 0, 0, 0])\n","    for i, (x, x_p) in enumerate(zip(label_tuple, predict_tuple)):\n","        if point_equal(x, x_p, tolerte):\n","            n[i] = 1\n","    return n\n","\n","\n","def plotlabel(y, bias):\n","    cmap = ['k', 'r', 'g', 'b', 'c', 'y']\n","    start = end = 0\n","    for i in range(len(y) - 1):\n","        if y[i] != y[i + 1]:\n","            end = i\n","            plt.plot(np.arange(start, end), y[start:end] - bias, cmap[int(y[i])])\n","            start = i + 1\n","        if i == len(y) - 2:\n","            end = len(y) - 1\n","            plt.plot(np.arange(start, end), y[start:end] - bias, cmap[int(y[i])])\n","\n","\n","def caculate_error(label_tuple, predict_tuple):\n","    error = np.zeros((5,))\n","    for i, (x, x_p) in enumerate(zip(label_tuple, predict_tuple)):\n","        error[i] = (x - x_p)/250*100  # (ms)\n","    return error\n","\n"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":386},"id":"XX9e9xzzWmmf","executionInfo":{"status":"error","timestamp":1607416074330,"user_tz":-540,"elapsed":970,"user":{"displayName":"히로","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggye-RKGYCwEq4L84yAntlpVGdcpIrR0aiEf7tGjg=s64","userId":"15899289586719876657"}},"outputId":"085dccc1-a4cd-4fca-95a6-2183ade49ae3"},"source":["# loading data\n","ecg_train_db = ECGDataset(qtdb_pkl, 'train_data.pkl')\n","ecg_train_dl = DataLoader(ecg_train_db, batch_size=BATCH_SIZE,\n","                          shuffle=True, num_workers=1)\n","\n","ecg_val_db = ECGDataset(qtdb_pkl, 'val_data.pkl')\n","ecg_val_dl = DataLoader(ecg_val_db, batch_size=BATCH_SIZE,\n","                        shuffle=False, num_workers=1)\n","\n","if TRAIN:\n","    if CONTINUE_TRAIN:\n","        # continue training\n","        net = restore_net(save_path + 'epoch_102.ckpt')\n","    else:\n","        # model\n","        net = SegModel(input_size=2, hidden_size=32, num_layers=2, out_size=NUM_SEGS_CLASS)\n","\n","    optimizer = torch.optim.Adam(net.parameters(), lr=1e-3)\n","    criterion = nn.CrossEntropyLoss()\n","    optimizer.zero_grad()\n","\n","    train(net, ecg_train_dl, EPOCHS)\n","\n","if TEST:\n","    # vis\n","    net = restore_net(save_path+'epoch_99.ckpt')\n","    net.eval()\n","    # test(ecg_val_dl, 'val', 4)\n","    for i, idx in enumerate([20, 60,\n","                              160, 280]):\n","        sample = ecg_val_db[idx]\n","        signal = sample['signal'].numpy()\n","        label = sample['label'].numpy()\n","        # plotecg(signal, label, 0, 1300)\n","        output = net(sample['signal'].unsqueeze(0))\n","        _, predict = torch.max(output, 1)\n","        # 将predict 和 label画出来\n","        predict = predict.numpy()\n","        x = np.arange(len(predict))\n","        plt.subplot(2, 2, i+1)\n","        plt.plot(x, signal[:, 0])\n","        plotlabel(label, 0.2)\n","        plotlabel(predict, 0.4)\n","    plt.show()\n","\n","if PAPER_TEST:\n","    net = restore_net(save_path + 'epoch_99.ckpt')\n","    net.eval()\n","    print('waiting several minutes')\n","    right_point_num = np.array([0, 0, 0, 0, 0])\n","    error_array = np.zeros(shape=(len(ecg_val_db), 5))\n","    if SAVE_TEST_FIG:\n","        with PdfPages('test.pdf') as pdf:\n","            for i in range(len(ecg_val_db)):\n","                sample = ecg_val_db[i]\n","                signal = sample['signal'].numpy()\n","                label = sample['label'].numpy()\n","                # 得到预测结果\n","                output = net(sample['signal'].unsqueeze(0))\n","                _, predict = torch.max(output, 1)\n","                predict = predict.numpy()\n","\n","                x = np.arange(Seqlength)\n","                plt.plot(x, signal[:, 0])\n","                plotlabel(label, 0.2)\n","                plotlabel(predict, 0.4)\n","                pdf.savefig()\n","                plt.close()\n","\n","                label_points = get_charateristic(label)\n","                predict_points = get_charateristic(predict)\n","\n","                error_array[i] = caculate_error(label_points, predict_points)\n","\n","                # 得到p-end, QRS onset end , T-middle\n","                right_point_num += right_point(label_points,\n","                                                predict_points, 0.016)\n","    else:\n","        for i in range(len(ecg_val_db)):\n","            sample = ecg_val_db[i]\n","            signal = sample['signal'].numpy()\n","            label = sample['label'].numpy()\n","            # 得到预测结果\n","            output = net(sample['signal'].unsqueeze(0))\n","            _, predict = torch.max(output, 1)\n","            predict = predict.numpy()\n","            # 得到p-end, QRS onset end , T-middle\n","            right_point_num += right_point(get_charateristic(label),\n","                                            get_charateristic(predict), 0.025)\n","            \n","means = np.mean(error_array, axis=0)\n","SD = np.std(error_array, axis=0)\n","print(means)\n","print(SD)\n","print(right_point_num/len(ecg_val_db))"],"execution_count":11,"outputs":[{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-11-871cce46bbaf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mPAPER_TEST\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m     \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrestore_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'epoch_99.ckpt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m     \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'waiting several minutes'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-4-ea424b66728c>\u001b[0m in \u001b[0;36mrestore_net\u001b[0;34m(ckpt)\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[0;31m# load models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mckpt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m         \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module)\u001b[0m\n\u001b[1;32m    259\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    262\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnew_fd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_load\u001b[0;34m(f, map_location, pickle_module)\u001b[0m\n\u001b[1;32m    407\u001b[0m     \u001b[0munpickler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnpickler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m     \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpersistent_load\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpersistent_load\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 409\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    410\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m     \u001b[0mdeserialized_storage_keys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mpersistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m    372\u001b[0m             \u001b[0;31m# Ignore containers that don't have any sources saved\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 374\u001b[0;31m                 \u001b[0m_check_container_source\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    375\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mtypename\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'storage'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_check_container_source\u001b[0;34m(container_type, source_file, original_source)\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_check_container_source\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontainer_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_source\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m         \u001b[0mcurrent_source\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetsource\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontainer_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    285\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moriginal_source\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mcurrent_source\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcontainer_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump_patches\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.6/inspect.py\u001b[0m in \u001b[0;36mgetsource\u001b[0;34m(object)\u001b[0m\n\u001b[1;32m    971\u001b[0m     \u001b[0;32mor\u001b[0m \u001b[0mcode\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m  \u001b[0mThe\u001b[0m \u001b[0msource\u001b[0m \u001b[0mcode\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mreturned\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0ma\u001b[0m \u001b[0msingle\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m.\u001b[0m  \u001b[0mAn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    972\u001b[0m     OSError is raised if the source code cannot be retrieved.\"\"\"\n\u001b[0;32m--> 973\u001b[0;31m     \u001b[0mlines\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlnum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetsourcelines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    974\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    975\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.6/inspect.py\u001b[0m in \u001b[0;36mgetsourcelines\u001b[0;34m(object)\u001b[0m\n\u001b[1;32m    953\u001b[0m     raised if the source code cannot be retrieved.\"\"\"\n\u001b[1;32m    954\u001b[0m     \u001b[0mobject\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 955\u001b[0;31m     \u001b[0mlines\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlnum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfindsource\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    956\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    957\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mistraceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.6/inspect.py\u001b[0m in \u001b[0;36mfindsource\u001b[0;34m(object)\u001b[0m\n\u001b[1;32m    766\u001b[0m     is raised if the source code cannot be retrieved.\"\"\"\n\u001b[1;32m    767\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 768\u001b[0;31m     \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetsourcefile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    769\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m         \u001b[0;31m# Invalidate cache if needed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.6/inspect.py\u001b[0m in \u001b[0;36mgetsourcefile\u001b[0;34m(object)\u001b[0m\n\u001b[1;32m    682\u001b[0m     \u001b[0mReturn\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mno\u001b[0m \u001b[0mway\u001b[0m \u001b[0mcan\u001b[0m \u001b[0mbe\u001b[0m \u001b[0midentified\u001b[0m \u001b[0mto\u001b[0m \u001b[0mget\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    683\u001b[0m     \"\"\"\n\u001b[0;32m--> 684\u001b[0;31m     \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    685\u001b[0m     \u001b[0mall_bytecode_suffixes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmachinery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDEBUG_BYTECODE_SUFFIXES\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    686\u001b[0m     \u001b[0mall_bytecode_suffixes\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmachinery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOPTIMIZED_BYTECODE_SUFFIXES\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.6/inspect.py\u001b[0m in \u001b[0;36mgetfile\u001b[0;34m(object)\u001b[0m\n\u001b[1;32m    652\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__file__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    653\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__file__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 654\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{!r} is a built-in class'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    655\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mismethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    656\u001b[0m         \u001b[0mobject\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__func__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: <module '__main__'> is a built-in class"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"u9LBNAbNZbth","executionInfo":{"status":"ok","timestamp":1607414967471,"user_tz":-540,"elapsed":7930,"user":{"displayName":"히로","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggye-RKGYCwEq4L84yAntlpVGdcpIrR0aiEf7tGjg=s64","userId":"15899289586719876657"}},"outputId":"e49727b4-1efd-4eb2-d401-d2ab7000cda3"},"source":["!pip install wfdb"],"execution_count":7,"outputs":[{"output_type":"stream","text":["Collecting wfdb\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b1/a0/922d06ec737e219a9f45545432842e68a84e8b52f292704056eea1d35e41/wfdb-3.1.1.tar.gz (113kB)\n","\u001b[K     |████████████████████████████████| 122kB 12.7MB/s \n","\u001b[?25hRequirement already satisfied: certifi>=2016.8.2 in /usr/local/lib/python3.6/dist-packages (from wfdb) (2020.11.8)\n","Requirement already satisfied: chardet>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from wfdb) (3.0.4)\n","Requirement already satisfied: cycler>=0.10.0 in /usr/local/lib/python3.6/dist-packages (from wfdb) (0.10.0)\n","Requirement already satisfied: idna>=2.2 in /usr/local/lib/python3.6/dist-packages (from wfdb) (2.10)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from wfdb) (0.17.0)\n","Requirement already satisfied: kiwisolver>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from wfdb) (1.3.1)\n","Requirement already satisfied: matplotlib>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from wfdb) (3.2.2)\n","Collecting mne>=0.18.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4d/0e/6448521738d3357c205795fd5846d023bd7935bb83ba93a1ba0f7124205e/mne-0.21.2-py3-none-any.whl (6.8MB)\n","\u001b[K     |████████████████████████████████| 6.8MB 3.3MB/s \n","\u001b[?25hCollecting nose>=1.3.7\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/15/d8/dd071918c040f50fa1cf80da16423af51ff8ce4a0f2399b7bf8de45ac3d9/nose-1.3.7-py3-none-any.whl (154kB)\n","\u001b[K     |████████████████████████████████| 163kB 52.9MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.10.1 in /usr/local/lib/python3.6/dist-packages (from wfdb) (1.18.5)\n","Requirement already satisfied: pandas>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from wfdb) (1.1.4)\n","Requirement already satisfied: pyparsing>=2.0.4 in /usr/local/lib/python3.6/dist-packages (from wfdb) (2.4.7)\n","Requirement already satisfied: python-dateutil>=2.4.2 in /usr/local/lib/python3.6/dist-packages (from wfdb) (2.8.1)\n","Requirement already satisfied: pytz>=2018.3 in /usr/local/lib/python3.6/dist-packages (from wfdb) (2018.9)\n","Requirement already satisfied: requests>=2.8.1 in /usr/local/lib/python3.6/dist-packages (from wfdb) (2.23.0)\n","Requirement already satisfied: scikit-learn>=0.18 in /usr/local/lib/python3.6/dist-packages (from wfdb) (0.22.2.post1)\n","Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from wfdb) (1.4.1)\n","Requirement already satisfied: six>=0.9.0 in /usr/local/lib/python3.6/dist-packages (from wfdb) (1.15.0)\n","Requirement already satisfied: sklearn>=0.0 in /usr/local/lib/python3.6/dist-packages (from wfdb) (0.0)\n","Collecting threadpoolctl>=1.0.0\n","  Downloading https://files.pythonhosted.org/packages/f7/12/ec3f2e203afa394a149911729357aa48affc59c20e2c1c8297a60f33f133/threadpoolctl-2.1.0-py3-none-any.whl\n","Requirement already satisfied: urllib3>=1.22 in /usr/local/lib/python3.6/dist-packages (from wfdb) (1.24.3)\n","Building wheels for collected packages: wfdb\n","  Building wheel for wfdb (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for wfdb: filename=wfdb-3.1.1-cp36-none-any.whl size=117829 sha256=02afc4c28ae8d4daaf8f1f62ce7b5a6ad801e26453708b82c1377598b7dc1975\n","  Stored in directory: /root/.cache/pip/wheels/bc/d0/c1/90538d266ccba2d1076fbc9970192c7ea1a09c99df3e65c69b\n","Successfully built wfdb\n","Installing collected packages: mne, nose, threadpoolctl, wfdb\n","Successfully installed mne-0.21.2 nose-1.3.7 threadpoolctl-2.1.0 wfdb-3.1.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"09xUujvgY9HE","executionInfo":{"status":"ok","timestamp":1607414972924,"user_tz":-540,"elapsed":4665,"user":{"displayName":"히로","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggye-RKGYCwEq4L84yAntlpVGdcpIrR0aiEf7tGjg=s64","userId":"15899289586719876657"}}},"source":["#####################\n","### qtdatabase.py ###\n","#####################\n","\n","import os\n","import math\n","import wfdb\n","import pickle\n","import numpy as np\n","import scipy.stats as st\n","import matplotlib.pyplot as plt\n","from scipy.signal import medfilt\n","from matplotlib.backends.backend_pdf import PdfPages"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":239},"id":"yebYU0jaZaaQ","executionInfo":{"status":"error","timestamp":1607415056227,"user_tz":-540,"elapsed":1610,"user":{"displayName":"히로","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggye-RKGYCwEq4L84yAntlpVGdcpIrR0aiEf7tGjg=s64","userId":"15899289586719876657"}},"outputId":"969e5b5f-5798-46f4-dfe1-e536d498573c"},"source":["Z_SCORE = True  # 对训练数据是否进行zscore归一化\n","SAVE_FIG = True  # 存储训练信号图像100个\n","train_percentage = 0.7  # 训练数据比例\n","features = 2  # 特征数目，用一条导联就是1，用两条导联就是2\n","Seqlength = 300  # 按照300个采样点\n","qtdbpath = './qtdb/'  # 数据路径\n","ann_suffix = 'q1c'  # 标注文件的后缀\n","qtdb_pickle_save = '/content/drive/MyDrive/2020-2 Deep Learning/팀프로젝트/[LAST] ECG-Segment-LSTM-master/qtdb_pkl/'  # 经过处理后保存数据路径\n","\n","if not os.path.exists(qtdb_pickle_save):\n","    os.mkdir(qtdb_pickle_save)\n","\n","# 下面几个文件没有P波，不参与本次实验\n","exclude = set()\n","exclude.update([\"sel35\", \"sel36\", \"sel37\", \"sel50\",\n","                \"sel102\", \"sel104\", \"sel221\",\n","                \"sel232\", \"sel310\"])\n","# 过滤不参与本次实验的record\n","datafiles = [x[:-4] for x in os.listdir(qtdbpath) if x[-4:] == '.dat']\n","record_names = list(set(datafiles)-exclude)\n","\n","\n","def baseline_correction(signals):\n","    \"\"\"\n","    使用2个中值滤波器获得baseline,这个由于数据点数比较多，处理可能稍微慢一点\n","    中值滤波器的窗口分别为50=250*0.2s\n","                      150=250*0.6s\n","                      中值滤波需要奇数 所以是49 149\n","    :param signals：采样点信号 shape为(N,)\n","                   输入单导联体信号\n","    :return:去除基线后的采样信号\n","    \"\"\"\n","    base_line = medfilt(signals, 49)\n","    base_line = medfilt(base_line, 149)\n","    signals = signals - base_line\n","    return signals\n","\n","\n","def ecg_preprocess(record):\n","    \"\"\"\n","    过滤噪声的滤波器会对信号造成一定损失，为了保证数据的完整性，或者说\n","    检验网络的鲁棒性，我们不做噪声去除。\n","    :param record:\n","    :return:\n","    \"\"\"\n","    record[:, 0] = baseline_correction(record[:, 0])\n","    record[:, 1] = baseline_correction(record[:, 1])\n","    return record\n","\n","\n","def remove_seq_gaps(x, y):\n","    \"\"\"\n","    去掉非正常标注的片段， 如何衔接需要优化\n","    :param x:\n","    :param y:\n","    :return:\n","    \"\"\"\n","    window = 150\n","    c = 0\n","    include = []\n","    print(\"filterering.\")\n","    print(\"before shape x,y\", x.shape, y.shape)\n","    for i in range(y.shape[0]):\n","        # 将连续未标注的超过150个点的整个未标注的去掉\n","        if 0 < c < window and y[i] != 0:\n","            for t in reversed(range(c)):\n","                include.append(i-t-1)\n","            c = 0\n","            include.append(i)\n","        elif c >= window and y[i] != 0:\n","            include.append(i)\n","            c = 0\n","        elif y[i] == 0:\n","            c += 1\n","        else:\n","            include.append(i)\n","    x, y = x[include, :], y[include]\n","    print(\" after shape x,y\", x.shape, y.shape)\n","    return x, y\n","\n","\n","def calculate_interv(poses):\n","    pt_interv = 10000\n","    pt_len = 0\n","    pp_interv = 10000\n","    for i, pose in enumerate(poses):\n","        if i < len(poses)-1:\n","            pt_interv = min(pt_interv, poses[i+1][0]-pose[-1])\n","            pp_interv = min(pp_interv, poses[i+1][0]-pose[0])\n","        pt_len = max(pt_len, pose[-1]-pose[0])\n","    return pt_interv, pt_len, pp_interv\n","\n","\n","def splitseg_single_beat(signal, label, poses):\n","    xx = np.zeros((len(poses), Seqlength, signal.shape[1]))\n","    yy = np.zeros((len(poses), Seqlength))\n","    for i, pose in enumerate(poses):\n","        x = np.zeros((Seqlength, features))\n","        y = np.zeros((Seqlength,))\n","        pstart, tend = pose[0], pose[-1]\n","        len_beat = tend-pstart+1\n","        start = (Seqlength-len_beat)//2\n","        x[start:start+len_beat] = signal[pstart:tend+1, :]\n","        y[start:start+len_beat] = label[pstart:tend+1]\n","        xx[i] = x\n","        yy[i] = y\n","    return xx, yy\n","\n","\n","def splitseg(signal, label, num, overlap):\n","    \"\"\"\n","    创建LSTM训练和验证使用的片段，长度为num+2*overlap\n","    :param signal:\n","    :param label:\n","    :param num:\n","    :param overlap:\n","    :return:\n","    \"\"\"\n","    length = signal.shape[0]\n","    num_seg = math.ceil(length / num)  # 计算可以得到多少个数据片段, 向上取整可能不是很合适，原因见下面的shape检查\n","    upper = num_seg * num  # math.ceil(8.5)=9\n","    print(\"splitting on\", num, \"with overlap of \", overlap, \"total datapoints:\", signal.shape[0], \"; upper:\", upper)\n","    xx = np.empty((num_seg, num + 2 * overlap, signal.shape[1]))  # 训练数据\n","    yy = np.empty((num_seg, num + 2 * overlap, ))  # 标签\n","    # 第一个片段取前num+overlap个 然后再在前面补overlap个零\n","    # 最后一个片段取后面num+overlap个 然后再在后面补overlap个零\n","    for i in range(num_seg):\n","        if i == 0:\n","            tmp = np.zeros((num+2*overlap, signal.shape[1]))\n","            tmp[overlap:, :] = signal[:num+overlap, :]\n","        elif i == num_seg-1:\n","            tmp = np.zeros((num+2*overlap, signal.shape[1]))\n","            tmp[:num+overlap, :] = signal[-(num+overlap):, :]\n","        else:\n","            # shape 检查，如果小于(num+2overlap,),则后面补零,这种情况会出现在7089扩充到8000\n","            tmp = np.zeros((num + 2 * overlap, signal.shape[1]))\n","            signal_i = signal[i*num-overlap: ((i+1)*num+overlap), :]\n","            tmp[:signal_i.shape[0]] = signal_i\n","        xx[i] = tmp\n","\n","    for i in range(num_seg):\n","        if i == 0:\n","            tmp = np.zeros((num+2*overlap, ))\n","            tmp[overlap:] = label[:num+overlap]\n","        elif i == num_seg-1:\n","            tmp = np.zeros((num+2*overlap, ))\n","            tmp[:num+overlap] = label[-(num+overlap):]\n","        else:\n","            # shape 检查，如果小于(num+2overlap,),则后面补零,这种情况会出现在7089扩充到8000\n","            tmp = np.zeros((num + 2 * overlap, ))\n","            label_i = label[i*num-overlap: ((i+1)*num+overlap)]\n","            tmp[:label_i.shape[0]] = label_i\n","        yy[i] = tmp\n","    return xx, yy\n","\n","\n","def plotecg(x, y, start, end):\n","    x = x[start:end, 0]  # 只取第一条信号\n","    y = y[start:end]\n","    cmap = ['k', 'r', 'g', 'b']\n","    start = end = 0\n","    for i in range(len(y)-1):\n","        if y[i] != y[i+1]:\n","            end = i\n","            plt.plot(np.arange(start, end+1), x[start:end+1], cmap[int(y[i])])\n","            start = i+1\n","    plt.show()\n","\n","\n","def plotlabel(y, bias):\n","    cmap = ['k', 'r', 'g', 'b', 'c', 'y']\n","    start = end = 0\n","    for i in range(len(y) - 1):\n","        if y[i] != y[i + 1]:\n","            end = i\n","            plt.plot(np.arange(start, end), y[start:end]-bias, cmap[int(y[i])])\n","            start = i + 1\n","        if i == len(y)-2:\n","            end = len(y)-1\n","            plt.plot(np.arange(start, end), y[start:end] - bias, cmap[int(y[i])])\n","\n","\n","x = np.zeros((1, Seqlength, features))\n","y = np.zeros((1, Seqlength,))\n","\n","min_tp = min_pp = 10000\n","max_len = 0\n","\n","for record_name in record_names:\n","    # 先读标注文件，再根据标注文件的长度来读record\n","    annotation = wfdb.rdann(qtdbpath+record_name, extension=ann_suffix)\n","    start = annotation.sample[0]\n","    end = annotation.sample[-1]\n","    print('record {} start,end: {}, {}'.format(record_name, start, end))\n","    record, _ = wfdb.rdsamp(qtdbpath+record_name, sampfrom=start, sampto=end+1)\n","\n","    # 两个信号都当做特征，所以每一个采样点2个特征\n","    signal = ecg_preprocess(record)\n","\n","    # 将x进行zscore归一化\n","    if Z_SCORE:\n","        for i in range(signal.shape[1]):\n","            signal[:, i] = st.zscore(signal[:, i])\n","\n","    Ann = list(zip(annotation.sample, annotation.symbol))\n","    poses = []\n","    for i in range(len(Ann)):\n","        ann = Ann[i]\n","        # 先找到P波,根据p波查找整个波形\n","        if ann[1] == 'p':\n","            pstart = ppos = pend = qpos = rpos = spos = tpos = tend = 0\n","            # 确定p波的起始和结束位置\n","            ppos = Ann[i][0]  # p波点\n","            if Ann[i - 1][1] == '(':\n","                pstart = Ann[i - 1][0]\n","            if Ann[i + 1][1] == ')':\n","                pend = Ann[i + 1][0]\n","            # p波紧随其后的就是QRS， 确定QRS波的位置\n","            if Ann[i + 3][1] == 'N':\n","                rpos = Ann[i + 3][0]\n","                if Ann[i + 2][1] == '(':\n","                    qpos = Ann[i + 2][0]\n","                if Ann[i + 4][1] == ')':\n","                    spos = Ann[i + 4][0]\n","                # 确认t波，因为有的没有‘(’,分情况讨论\n","                # 为了标注统一，只用t - ')'的信息,半个t wave\n","                if Ann[i + 6][1] == 't':\n","                    if Ann[i + 5][1] == '(':\n","                        # tpos = Ann[i + 5][0]\n","                        tpos = Ann[i + 6][0]\n","                    if Ann[i + 7][1] == ')':\n","                        tend = Ann[i + 7][0]\n","                elif Ann[i + 5][1] == 't':\n","                    tpos = Ann[i + 5][0]\n","                    if Ann[i + 6][1] == ')':\n","                        tend = Ann[i + 6][0]\n","                else:\n","                    print(\"can't find t wave\")\n","            poses.append((pstart - start, ppos - start, pend - start, qpos - start,\n","                          rpos - start, spos - start, tpos - start, tend - start))\n","    label = np.zeros((end - start + 1))\n","    for pose in poses:\n","        (pstart, ppos, pend, qpos, rpos, spos, tpos, tend) = pose\n","        label[ppos: pend] = 1  # half P Wave\n","        label[qpos: rpos] = 2  # QR\n","        label[rpos: spos] = 3  # RS\n","        label[tpos: tend] = 4  # half t Wave\n","\n","    # 计算相邻两个片段的min(前一个波的tend与后一个波的pstart的距离)\n","    # 计算一个片段的最大pt长度\n","    # 计算相邻两个片段的min(前一个波的pstart与后一个波的pstart的距离)\n","    # 这3个信息将用于决定，我们如何分割片段。\n","    # 该代码只使用一次。\n","    # 我们得到min_tp:1, max_len:283, min_pp:113 ，可以发现最小的tp间隔只有 1\n","    # 因为标注的片段是分散的，为了避免引入未标注数据，决定仅仅划分标注的片段。\n","    # 固定长度为300， 不到该长度的前后补零\n","    # pt_interv, pt_len, pp_interv = calculate_interv(poses)\n","    # min_tp = min(min_tp, pt_interv)\n","    # max_len = max(max_len, pt_len)\n","    # min_pp = min(min_pp, pp_interv)\n","\n","    # 将过滤前后的信号与标注图画出来\n","    # plotecg(signal, label, 0, 500)\n","    # signal, label = remove_seq_gaps(signal, label)\n","    # plotecg(signal, label, 0, len(label))\n","    xx, yy = splitseg_single_beat(signal, label, poses)\n","    # xx, yy = splitseg(signal, label, 1000, 150)\n","    x = np.vstack((x, xx))\n","    y = np.vstack((y, yy))\n","\n","print(\"min_tp:{}, max_len:{}, min_pp:{}\".format(min_tp, max_len, min_pp))\n","\n","# 将初始化的第一个sample去掉, 然后将片段打乱\n","x, y = x[1:], y[1:]\n","assert len(x) == len(y)\n","p = np.random.permutation(range(len(x)))\n","x, y = x[p], y[p]\n","\n","# 划分训练集和验证集，然后保存下\n","nums = len(x)\n","train_len = int(math.ceil(nums*train_percentage))\n","x_train, y_train = x[:train_len], y[:train_len]\n","x_val, y_val = x[train_len:], y[train_len:]\n","\n","print('训练集共有{}个片段，验证集共有{}个片段'.format(train_len, nums-train_len))\n","\n","if SAVE_FIG:\n","    with PdfPages(qtdb_pickle_save+'example.pdf') as pdf:\n","        for i in range(100):\n","            signal = x_train[i][:, 0]\n","            x = np.arange(Seqlength)\n","            plt.plot(x, signal)\n","            plotlabel(y_train[i], 0.2)\n","            pdf.savefig()\n","            plt.close()\n","\n","with open(qtdb_pickle_save+'train_data.pkl', 'wb') as f:\n","    pickle.dump((x_train, y_train), f)\n","\n","with open(qtdb_pickle_save+'val_data.pkl', 'wb') as f:\n","    pickle.dump((x_val, y_val), f)"],"execution_count":10,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-10-f48aaa50b9cb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m                 \"sel232\", \"sel310\"])\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# 过滤不参与本次实验的record\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mdatafiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqtdbpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'.dat'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0mrecord_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatafiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mexclude\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/2020-2 Deep Learning/팀프로젝트/[LAST] ECG-Segment-LSTM-master/qtdb/'"]}]},{"cell_type":"code","metadata":{"id":"ReL8hzn5ZlmP"},"source":[""],"execution_count":null,"outputs":[]}]}